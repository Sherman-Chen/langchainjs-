# 模型相关

## token

"token"（标记）通常指的是一种语言中的基本单位，可以是一个单词、一个字符或者一个子词。在自然语言处理（NLP）中，文本通常会被分割成标记，以便进行语言理解和处理。

例如，在英语中，句子 "ChatGPT is amazing!" 可以被分割成以下标记：

"ChatGPT"
"is"
"amazing"
"!"
每个单词和标点符号都是一个标记。在某些情况下，还可以使用子词作为标记，尤其是在处理语言中的复杂形态时，比如将单词拆分成词根和后缀。

在使用LLM时，模型通常按照标记的顺序来理解和生成文本。这有助于模型更好地理解语言的结构和语境，从而更有效地执行各种自然语言处理任务。

类似于UTF8，每个中文字都有一个编码。token就是另外一种编码，是专门给大模型用的。

不同模型服务商，他们对同一个字生成的token可能不一样。比如对于“你”这个字，A服务商生成的token可能是1，B服务商可能是2。

大模型的计费的基本单位都是以token来计算。

## embedding
"embedding" 通常指的是将词语或其他离散型数据映射到连续向量空间的过程。这个过程将每个词语表示为一个固定长度的向量，使得语言模型能够更好地理解和处理这些词语之间的语义关系。

举个例子，考虑以下两个句子：“猫喜欢吃鱼”和“狗喜欢吃肉”。在词嵌入中，模型会为每个词（如“猫”、“狗”、“喜欢”、“吃”、“鱼”、“肉”）分配一个向量。这些向量的选择是通过模型在大量文本数据上学到的，使得相似的词在向量空间中更加接近。

假设我们的词向量是4维的，那么可以用如下方式表示这两个句子的一部分词语的词嵌入：

猫（Cat）: [0.2, 0.5, -0.1, 0.8]
狗（Dog）: [0.3, 0.4, -0.2, 0.7]
喜欢（Like）: [0.9, 0.6, -0.5, 0.3]
吃（Eat）: [0.7, 0.2, -0.8, 0.6]
鱼（Fish）: [-0.4, 0.9, 0.1, -0.7]
肉（Meat）: [-0.6, 0.7, 0.4, -0.5]
这样，通过词嵌入，模型可以将每个词语转换成一个具有连续性结构的向量，从而更好地捕捉它们之间的语义关系。

可以简单理解成，embedding就是一个float的数组，这个数组表示了这个句子在多维空间上面某个点，这个点表示这段话的语义。

通过计算两个句子的embedding的cos值，便能够知道两个句子的语义相似的，也就是两个句子表达的意思是不是差不多。

## decoder-only vs encoder-only
* Decoder-only模型：这种模型只有一个解码器，它可以根据给定的输入或者自身的隐藏状态生成输出序列。它们擅长写出吸引人和有信息量的内容，但是不擅长理解输入的主题和目标。一些Decoder-only模型的例子是GPT系列模型，如GPT-31。
* Encoder-only模型：这种模型只有一个编码器，它可以根据输入序列生成一组隐藏状态，表示输入的语义和上下文。它们擅长理解输入的关系和含义，但是不擅长生成内容。一个Encoder-only模型的例子是BERT2。
* Encoder-Decoder模型：这种模型包含一个编码器和一个解码器，它们可以协同工作，根据输入序列生成输出序列。它们可以处理输入和输出之间的对应关系，同时保持内容的一致性和连贯性。一些Encoder-Decoder模型的例子是BART和T5

简单说就是decode-only是擅长生成内容，encoder-only则是擅长理解内容。

## prompt
prompt是一种向语言模型或AI系统提供有效的输入提示或指令的方法。

这些提示在引导模型的响应和控制模型的行为方面起着关键作用。

设计良好的提示可以帮助研究人员和开发人员影响AI系统的输出，以实现期望的目标和提高模型的性能

### context
预训练的LLM在一般的自然语言任务上表现很好，甚至只需要一个简短的提示，比如一个需要补全的句子或一个问题，这就是所谓的零样本学习(zero-shot)。

然而，用户如果能够更详细地描述他们的查询，提供一些请求和例子，就能够获得更准确和更符合期望的答案。这种情况下，我们称之为“单样本”学习，表示提示只包含一个例子(one-shot)。

(few-shot)则是提示中包含多个例子。

> 通常给出例子的时候，也会对模型造成干扰。因此通常还需要在prompt中强调例子只是例子而已，不要跟例子中完全一样。

## RAG（Retrieval Augmented Generation）
中文意思就是检索增强生成，意思是通过检索模型外的内容增强模型生成的质量。

LLM在生成答案时只能使用它们训练时的数据，这意味着它们不知道训练后发生的事实，也不能访问非公开信息（如公司数据，当前的新闻）。

RAG是一种通过外部数据来增强提示的技术，外部数据以文档片段的形式呈现，考虑到提示长度的限制。这种技术由向量数据库工具支持，它们从各种预定义的数据源中检索有用的片段，并将它们添加到提示上下文中。

RAG非常适合那些没有足够的数据、时间或资源来微调LLM，但仍希望在特定的工作负载上提高性能并降低虚假内容的风险。

现在大模型的输入prompt趋势是越来越大，比如GPT4能够达到128K，gemini能达到100w。但是输入的prompt越大，token也越多，价格也越昂贵。

因此在目前阶段，RAG还是非常有必要的。

## MMR（maximal marginal relevance）
中文意思是最大相关性。具体来说，是使用了余弦相似度来度量输入与例子之间的语义相似性。它通过以下步骤实现：

1. 传入多个示例，然后计算每个例子的嵌入（embeddings），比如使用 OpenAI 的嵌入模型来为输入生成嵌入。

2. 使用余弦相似度测量输入与每个例子之间的相似性，找到与输入最相似的例子。

3. 组合成最终的prompt

用到的技术跟RAG差不多，RAG是计算资料的embedding，然后使用cos来计算资料与问题的相关性，得到相关资料。

这里用到的是例子，实际上例子也可以当成资料。

> 实际应用中，这种方式比较少用。

# 结构化生成结果
## function call
大模型的输出通常也是一段文本，但是这个文本不利于我们进行编程。

如果要编程，我们需要将这个结果文本进行结构化。比如：

```
用户：帮我写一段用于宣传我的软件的标题和内容
大模型：
标题：xxx
内容：xxxx
```

如果我们要利用这个结果编程的话，那么需要自己去解析这个标题和内容。有人可能会以为那简单，直接用正则匹配一下不就可以吗？

但是大模型本身是有一定的不稳定性的，因此结果可能是生成：

```
标题xxx
内容xxx
```

因此我们需要一种能够让大模型生成稳定结构的方式。而fc(function call)就是这样的一种方式。

在让大模型进行生成的时候，我们将我们想要的结构传给大模型，大模型在生成结果的时候便会尽量按照我们的结果进行返回。

> 这里说的尽量，是因为大模型服务商也需要将我们的结构转换成prompt，而转换成的带有结构的prompt，服务商已经针对这种形式进行了训练，因此只是稳定性提高，但是并不是永远返回正确的结构，不过目前对于openai来说，其稳定性已经能够达到生产级别。

有了这个输出结构化结果的方式之后，我们就能够让模型做更多的事。比如我们将我们的API定义传给大模型，让大模型根据用户的问题选择特定的API执行。

这种方式也是function call这个名字的由来。

## tools
function call最早是由openai提供的一个参数，但是在1106版本之后，openai将这个参数名字改了下，改成了tools，因此tools与function call实际上是同一个东西。

## json mode
另外为了区分非function call的场景，比如说我不想要调用api，我只是想要得到结构化的结果。

在1106版本中，openai提供了另外一个参数，叫做reponse format，可以传入string或者是json object。

如果是json object的话，则是返回json这个结构化的版本。

> 1106版本中，因为openai出现了一个编码的bug，导致返回的结构化结果可能出现异常。这个在0125版本已经修复，因此如果使用的话，推荐使用0125版本。

简单总结一下，就是fc、tools和json mode其实都是为了返回结构化的结构。其中fc和tools通常用于定义API，而json mode则结构上更加自由一些。

> 我没有经过严格的测试，但是通过经验来说，json mode的生成质量会比fc(tools)更好，并且消耗的token也更少。但是结构化稳定性则是fc(tools)更好。我生产环境中，通常使用json mode。







